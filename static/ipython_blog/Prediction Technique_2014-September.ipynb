{
 "metadata": {
  "name": "",
  "signature": "sha256:84bde1d42266259f5ba74591c1902f0974b33f89c648d2e59b0c7138f02ede5f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Overfitting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Regularization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regularization is a process to introduce more information in a current model in order to avoid overfitting or ill-posed problem. It is useful for model selection reducing overfitting by adding a complexity penalty to the loss function and L1, L2 regularization are two common varaints adding to the machine learning algorithm. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take linear regression as an example, the cost function for linear regression is:\n",
      "\n",
      "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^i) - y^i)^2$$\n",
      "\n",
      "when the model is over-fitting, the cost function usually:\n",
      "\n",
      "$$ J(\\theta) \\approx 0 $$ \n",
      "\n",
      "but for the cross-validation cost function it will be huge otherwise. Regularization will avoid over-fitting, because it reduces magnitude of parameters when so many features in the model. Regularization is to penalize the cost function to make parameters smaller:\n",
      "\n",
      "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^i) - y^i)^2 + p(\\theta)$$\n",
      "\n",
      "where the penalty can be L1 or L2:\n",
      "\n",
      "$$p(\\theta) =\\frac{1}{2m} \\lambda \\sum_{i=1}^{m} \\theta_j$$ or\n",
      "\n",
      "$$p(\\theta) =\\frac{1}{2m} \\lambda \\sum_{i=1}^{m} \\theta^2_j$$\n",
      "\n",
      "One fact needs to be paid attention is not to penalize $\\theta_0$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dimensionality Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In machine learning area, there are many cases have high-dimensional measured input data. The reason why doing dimensionality reduction is:\n",
      "\n",
      "* Avoid redundancy and dependency across features. Some of the features with higher dimensionality will share common information with lower dimensional feature. Reduction will help generate smaller size training set. \n",
      "* Intrinsic structure discoery. \n",
      "* Removal of irrelevant and noisy features.\n",
      "* Visualization purpose."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Principal Component Analysis is most popular way to do the dimensionality reduction. \n",
      "\n",
      "Definition: Find a direction (a vector $u^{(1)} \\in \\mathbb{R}^n$) onto which to project the data so as to minimize the projection error. \n",
      "\n",
      "Procedure for reducing from $n$-dimensions to $k$-dimensions:\n",
      "\n",
      "1. Compute \"Covariance matrix\":\n",
      "$$\\Sigma = \\frac{1}{m}\\sum_{i=1}^{n}(x^{i})(x^{i})^T$$\n",
      "\n",
      "2. Compute \"eigenvectors\" of matrix $\\Sigma$:\n",
      "    \n",
      "        [U, S, V] = svd(Sigma)\n",
      "        \n",
      "3. Compute the lower dimensional matrix:\n",
      "$$ Z = (U^{k}) ^T \\times x $$\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In python scikit-learn package, you can directly use PCA function to apply: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.decomposition import PCA\n",
      "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(X)\n",
      "print(pca.explained_variance_ratio_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.99244289  0.00755711]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Choosing $k$:\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}