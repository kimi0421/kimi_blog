{
 "metadata": {
  "name": "",
  "signature": "sha256:189da0acda26ada2f43cebf6a5b6fa6bf5f5947ea234ff9a03dce2908b557a31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It just help me to remember some simple facts in statistics and machine learning. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Biased and Unbiased"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Biased:\n",
      "\n",
      "$$ \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
      "\n",
      "Unbiased:\n",
      "\n",
      "$$ \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
      "\n",
      "As far as I understand, biased and unbiased are just two ways to measure variance, there are no good or bad judgment. The reason why we usually use unbiased is unbiased variance has the same expected mean value.\n",
      "\n",
      "$$\\sum_{i=1}^{n}(x_i - \\bar{x})^2 = \\sum_{i=1}^{i=n}(x_i - \\mu)^2 - n (\\bar{x} - \\mu)^2$$\n",
      "\n",
      "Thus:\n",
      "\n",
      "$$\\sum_{i=1}^{n}(x_i - \\bar{x})^2 = n\\sigma^2 - n(\\frac{\\sigma^2}{n}) = (n-1)\\sigma^2$$ "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Univariate Kernel Density Estimator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The kernel estimate converges in probability to the true density. \n",
      "\n",
      "$$\\hat{f}(x, h) = \\frac{1}{nh}\\sum_{i=1}^{n}K(\\frac{x-X_i}{h})$$\n",
      "\n",
      "with kernel function $K$ and bandwidth $h$. \n",
      "\n",
      "Some kernels:\n",
      "\n",
      "$$K(x, p) = \\frac{(1-x^2)^p}{2^{2p+1}B(p+1, p+1)}1_{\\mid x \\mid<1}$$\n",
      "\n",
      "with $B(a, b) = \\Gamma(a)\\Gamma(b)/\\Gamma(a+b)$\n",
      "\n",
      "* $p=0$: Uniform kernel\n",
      "* $p=1$: Epanechnikov kernel\n",
      "* $p=2$: Biweight kernel"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Bias vs Variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Bias usually causes under fitting (Train error and cross-validation error are both high)\n",
      "* Variance usually causes over fitting (Train error is low but cross-validation error is high)\n",
      "* Bias relates to the ability of your model function to approximate the data, Variance on the other hand is about the stability of your model in response to new training examples."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Sigmoid function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also known as logistic function:\n",
      "\n",
      "$$ y = \\frac{1}{1 + e^{-x}}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}