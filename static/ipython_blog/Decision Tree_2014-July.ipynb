{
 "metadata": {
  "name": "",
  "signature": "sha256:81b39a315b6582f70d8ab33e0dac9d6e3f9e2917ccf1bba2da0b2d19fa637112"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "What is Decision Tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a simple word, decision tree method help you construct a 'tree map' to make a decision. It is a method for approximating discrete-valued functions that is robust to noisy data and capable of learning disjunctive expressions. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How does it work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a very simple example: assume species in the world can all be grouped by mammals and non-mammals, but when a new scientist find a new species, how can we tell whether it is mammal or not mammal? \\n\n",
      "\n",
      "This is what I am going to do:\n",
      "\n",
      "   1. List many 'Yes' or 'No' questions that can help specify whether it is mammal or non-mammal, such as is the speceis cold or warm-blooded? If it is cold, definitely not mammal. By carefully crafting questions about the attributes of the test record we will have anwser to that. \n",
      "   2. Use these questions apply the new species scientists find and specify what it is exactly \n",
      "    \n",
      "Step 1 we create a model (questions), and feed historical data into the model to find the pattern (Yes or No lead to mammal or not mammal). After step 1, we have so called 'trained' model (yes or no patterned questions') and then we do forecasting with the new data. \n",
      "\n",
      "The tree has three types of nodes:\n",
      "\n",
      "   * A root node that has no incoming edges and zero or more outgoing edges\n",
      "   * Internal nodes, each of which has exactly one incoming edge and two or more outgoing edges\n",
      "   * Leaf or terminal nodes, each of which has exactly one incoming edge and no outgoing edges\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to build a decision tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Algorithm called 'ID3' is usually used for decision tree. It constructs decision trees top-down, begining with the questions \"which attribute should be tested at the root of the tree?\" To anwser this question, each attribute is evaluated using a statical test to determine how well it alone classifies the training examples. \n",
      "\n",
      "We used ${Information Gained}$ to specifiy how good is the attribute. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Core Mathematics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Entropy:\n",
      "$$H(\\frac{p}{p+n}, \\frac{n}{p+n}) = - \\frac{p}{p+n} \\log_{2}{\\frac{p}{p+n}} - \\frac{n}{p+n} \\log_{2}{\\frac{n}{p+n}}$$\n",
      "\n",
      "where:\n",
      "$p$ is positive and $n$ is negative \n",
      "\n",
      "If there are three characteristics, it would be $\\pi_1$, $\\pi_2$, $\\pi_3$, then the entropy would be:\n",
      "\n",
      "$$H(\\pi_i) = - \\sum \\pi_i \\log \\pi_i$$\n",
      "\n",
      "Information gain ($I$) is:\n",
      "$$I(A) = H(\\frac{p}{p+n}, \\frac{n}{p+n}) - EH(A)$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have the training data set:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/decision_tree.png\" height=\"600\" width=\"600\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above case we have the decision 'Wait' or 'Not Wait', so they are $p$ and $n$, thus $p = n = 6$\n",
      "\n",
      "From above table we take two attributes, 'patrons' and 'type' as our example to see which attribute should be the root of our tree:\n",
      "\n",
      "$$H(\\frac{p}{p+n}, \\frac{n}{p+n}) = H(\\frac{6}{12}, \\frac{6}{12}) = 1$$\n",
      "\n",
      "$$I(Patrons) = 1 - [\\frac{2}{12} H (0, 1) + \\frac{4}{12} H(1, 0) + \\frac{6}{12} H(\\frac{2}{6}, \\frac{4}{6})] = 0.0541$$\n",
      "\n",
      "$$I(Type) = 1 - [\\frac{2}{12} H (\\frac{1}{2}, \\frac{1}{2}) + \\frac{2}{12} H(\\frac{1}{2}, \\frac{1}{2}) + \\frac{4}{12}H(\\frac{2}{4}, \\frac{2}{4}) + \\frac{4}{12} H(\\frac{2}{4}, \\frac{2}{4})] = 0$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using Python To Model Decision Tree"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Python Package"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the scikit-learn to model decision tree. Install the scikit-learn with pip. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "pip install scikit-learn"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "x = [[0, 0], [1, 1]]\n",
      "y = [0, 1]\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "clf = clf.fit(x, y)\n",
      "clf.predict([[2, 2]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "array([1])"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       1.00      1.00      1.00        50\n",
        "          2       1.00      1.00      1.00        50\n",
        "\n",
        "avg / total       1.00      1.00      1.00       150\n",
        "\n",
        "[[50  0  0]\n",
        " [ 0 50  0]\n",
        " [ 0  0 50]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on the previous decision tree example training data, we can use sklearn to find the decision tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "# Set up data\n",
      "\n",
      "y_train = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1])\n",
      "x_train = np.array([[1, 0, 0, 1, 1, 3, 0, 1, 1, 1], \n",
      "                    [1, 0, 0, 1, 2, 1, 0, 1, 2, 3], \n",
      "                    [0, 1, 0, 0, 1, 1, 0, 0, 3, 1], \n",
      "                    [1, 0, 1, 1, 2, 1, 0, 0, 2, 2],\n",
      "                    [1, 0, 1, 0, 2, 3, 0, 1, 1, 4],\n",
      "                    [0, 1, 0, 1, 1, 2, 1, 1, 4, 1], \n",
      "                    [0, 1, 0, 0, 0, 1, 1, 0, 3, 1],\n",
      "                    [0, 0, 0, 1, 1, 2, 1, 1, 2, 1], \n",
      "                    [0, 1, 1, 0, 2, 1, 1, 0, 3, 5], \n",
      "                    [1, 1, 1, 1, 2, 3, 0, 1, 4, 2],\n",
      "                    [0, 0, 0, 0, 0, 1, 0, 0, 2, 1], \n",
      "                    [1, 1, 1, 1, 1, 1, 0, 0, 3, 3]])\n",
      "\n",
      "# Train the model \n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(x_train, y_train)\n",
      "print model\n",
      "\n",
      "# Test the model, our case is:\n",
      "# Alt:T Bar: T, Friday: T, Hun: F, Pat: Full, Price: $$$\n",
      "# Rain: F, Res: F, Type: French, Est: 10-30\n",
      "\n",
      "# Result should be: Wait: 1\n",
      "\n",
      "attributes = np.array([1, 1, 1, 1, 2, 3, 0, 0, 1, 2])\n",
      "print model.predict(attributes)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DecisionTreeClassifier(compute_importances=None, criterion='gini',\n",
        "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            random_state=None, splitter='best')\n",
        "[1]\n"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}