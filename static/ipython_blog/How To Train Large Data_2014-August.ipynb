{
 "metadata": {
  "name": "",
  "signature": "sha256:540cb678b7bf6272e9391dd58c7e1a1217b4fc763991fa30443e0a2272923e1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Large Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When I anticipated in Kaggle display ads competition, I encountered a problem that how can I train the data that is larger than 10G. In machine learning, when we train the data we always read the training data set into memory and do optimization calculation to find the global best parameters. But what if you don't have large memory? Can we parallel the training method, even the simplest linear regression model? Can we use map-reduce method? Can we partially train the data and then average all the trained model generated? There are so many questions I have and I don't have very clear answer. I would like to do some research using python code to solve my problem. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I will try to use following strategy to solve the problem:\n",
      "\n",
      "* Use some models that supports partially trained\n",
      "* Train partial data and average all the models generated\n",
      "* Use spark to train and predict large data set"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SGD Regression "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than $10^5$ training examples and more than $10^5$ features. The disadvantages of Stochastic Gradient Descent include:\n",
      "\n",
      "* SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
      "* SGD is sensitive to feature scaling.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two cases for SGD implementation. \n",
      "\n",
      "1. All the variables are numerical variables\n",
      "2. Some variables are categorical variables\n",
      "\n",
      "For small data sets, these are no problem. But for the large data set, if some variables are categorical variables, how can I transfer those 'too many' categorical variables to numerical variables. As far as I know, couple methods are able to encode categorical variables to numerical variables:\n",
      "\n",
      "1. One-hot encoding (aka 1-of-K encoding)\n",
      "2. Feature hashing encoding\n",
      "\n",
      "One-hot encoding:\n",
      "\n",
      "    This method transforms each categorical feature with m possible values into m binary features, with only one active. For example, a person could have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1]. After one-hot encoding, [\"male\", \"from US\", \"uses Internet Explorer\"] will be [ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]\n",
      "    \n",
      "Feature hashing encoding:\n",
      "    \n",
      "    It is the easiest way to convert categorical variables to numerical variables. I follow the steps:\n",
      "    1. Use any hashing function to hash the feature. \n",
      "    2. Convert hashed string to 32bit integer. It is easy to implement and do not need large memory and easy to parallize. \n",
      "    \n",
      "From above analysis, one-hot encoding will create a very large matrix if the input data is too large, so I will choose feature hashing encoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}